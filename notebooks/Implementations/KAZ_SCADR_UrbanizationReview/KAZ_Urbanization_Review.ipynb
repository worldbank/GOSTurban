{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kazakhstan urbanization review\n",
    "\n",
    "In support of Giuseppe Rossitti and Tom Farole, we need to generate a set of standard urban analyses\n",
    "\n",
    "1. EC urban clusters using WorldPop 2020\n",
    "   - need to name the clusters as well\n",
    "2. Nighttime Lights for #1\n",
    "3. GHSL for #1\n",
    "4. Flood risk\n",
    "5. Variation in precipitation and temperature\n",
    "6. Air quality (PM 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import json\n",
    "import boto3\n",
    "import rasterio\n",
    "import folium\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import skimage.graph as graph\n",
    "\n",
    "from botocore.config import Config\n",
    "from botocore import UNSIGNED\n",
    "from shapely.geometry import Point, mapping\n",
    "from scipy import ndimage\n",
    "\n",
    "sys.path.insert(0, \"/home/wb411133/Code/GOSTNets_Raster/src\")\n",
    "import GOSTNetsRaster.market_access as ma\n",
    "# import GOSTNetsRaster.conversion_tables as speed_tables\n",
    "\n",
    "sys.path.insert(0, \"/home/wb411133/Code/gostrocks/src\")\n",
    "import GOSTrocks.rasterMisc as rMisc\n",
    "import GOSTrocks.ntlMisc as ntl\n",
    "from GOSTrocks.misc import tPrint\n",
    "\n",
    "sys.path.append(\"../../../src\")\n",
    "import GOST_Urban.UrbanRaster as urban\n",
    "import GOST_Urban.urban_helper as clippy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# read in local important parameters\n",
    "local_json = \"/home/wb411133/Code/urbanParameters.json\"\n",
    "with open(local_json, \"r\") as inJ:\n",
    "    important_vars = json.load(inJ)\n",
    "\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input data\n",
    "iso3 = \"KAZ\"\n",
    "global_population_file = (\n",
    "    \"/home/public/Data/GLOBAL/Population/WorldPop_PPP_2020/ppp_2020_1km_Aggregated.tif\"\n",
    ")\n",
    "global_population_ghs_file = \"/home/public/Data/GLOBAL/Population/GHS/2022_1km/GHS_POP_E2020_GLOBE_R2022A_54009_1000_V1_0.tif\"\n",
    "global_ghsl_folder = \"/home/public/Data/GLOBAL/GHSL/v2022/\"\n",
    "global_friction_surface = \"/home/public/Data/GLOBAL/INFRA/FRICTION_2020/2020_motorized_friction_surface.geotiff\"\n",
    "admin_bounds = \"/home/public/Data/COUNTRY/KAZ/ADMIN/kaz_districts.shp\"\n",
    "\n",
    "output_folder = \"/home/wb411133/projects/KAZ_SCADR_Urbanization\"\n",
    "output_data = os.path.join(output_folder, \"DATA\")\n",
    "worldpop_urban = os.path.join(output_data, \"WorldPop_Urban\")\n",
    "ghspop_urban = os.path.join(output_data, \"GHS_Urban\")\n",
    "ghsl_folder = os.path.join(output_data, \"GHSL\")\n",
    "ma_folder = os.path.join(output_data, \"MARKET_ACCESS\")\n",
    "\n",
    "if not os.path.exists(ma_folder):\n",
    "    os.makedirs(ma_folder)\n",
    "\n",
    "# Define output files\n",
    "local_population = os.path.join(output_data, f\"{iso3}_ppp_2020_1km_aggregated.tif\")\n",
    "local_ghs_population = os.path.join(output_data, f\"{iso3}_ghs_pop_2020.tif\")\n",
    "local_friction = os.path.join(output_data, f\"{iso3}_2020_motorized_travel.tif\")\n",
    "urban_tt_result = os.path.join(output_data, \"urban_travel_time.csv\")\n",
    "\n",
    "\"\"\"\n",
    "urban_extents_file        = os.path.join(worldpop_urban, f\"{iso3}_urban_extents.geojson\")\n",
    "urban_extents_raster_file = os.path.join(worldpop_urban, f\"{iso3}_urban_extents.tif\")\n",
    "urban_extents_hd_file     = os.path.join(worldpop_urban, f\"{iso3}_urban_extents_hd.geojson\")\n",
    "urban_extents_hd_raster_file = os.path.join(worldpop_urban, f\"{iso3}_urban_extents_hd.tif\")\n",
    "admin_urban_summary       = os.path.join(worldpop_urban, \"adm2_urban_summary.shp\")\n",
    "urban_admin_summary       = os.path.join(worldpop_urban, f\"{iso3}_ADM2_urban_summary.csv\")\n",
    "\"\"\"\n",
    "urban_extents_file = os.path.join(ghspop_urban, f\"{iso3}_urban_extents.geojson\")\n",
    "urban_extents_raster_file = os.path.join(ghspop_urban, f\"{iso3}_urban_extents.tif\")\n",
    "urban_extents_hd_file = os.path.join(ghspop_urban, f\"{iso3}_urban_extents_hd.geojson\")\n",
    "urban_extents_hd_raster_file = os.path.join(\n",
    "    ghspop_urban, f\"{iso3}_urban_extents_hd.tif\"\n",
    ")\n",
    "admin_urban_summary = os.path.join(ghspop_urban, \"adm2_urban_summary.shp\")\n",
    "urban_admin_summary = os.path.join(ghspop_urban, f\"{iso3}_ADM2_urban_summary.csv\")\n",
    "\n",
    "urban_viirs_summary = os.path.join(output_folder, f\"{iso3}_urban_viirs_summary.csv\")\n",
    "urban_hd_viirs_summary = os.path.join(\n",
    "    output_folder, f\"{iso3}_urban_hd_viirs_summary.csv\"\n",
    ")\n",
    "admin_viirs_summary = os.path.join(output_folder, f\"{iso3}_admin_viirs_summary.csv\")\n",
    "\n",
    "urban_ghsl_summary = os.path.join(output_folder, f\"{iso3}_urban_ghsl_summary.csv\")\n",
    "urban_hd_ghsl_summary = os.path.join(output_folder, f\"{iso3}_urban_hd_ghsl_summary.csv\")\n",
    "admin_ghsl_summary = os.path.join(output_folder, f\"{iso3}_admin_ghsl_summary.csv\")\n",
    "\n",
    "admin_final = os.path.join(output_folder, \"admin_summarized.shp\")\n",
    "urban_final = os.path.join(output_folder, \"urban_summarized.shp\")\n",
    "urban_hd_final = os.path.join(output_folder, \"urban_hd_summarized.shp\")\n",
    "focal_cities = os.path.join(output_folder, \"FOCAL_AOIs.shp\")\n",
    "\n",
    "# Define market access output\n",
    "all_routes_file = os.path.join(ma_folder, \"all_routes.shp\")\n",
    "time_matrix = os.path.join(ma_folder, \"all_routes_time_minutes.csv\")\n",
    "dist_matrix = os.path.join(ma_folder, \"all_routes_distance_km.csv\")\n",
    "dist_all_routes_file = os.path.join(ma_folder, \"all_routes.shp\")\n",
    "dist_time_matrix = os.path.join(ma_folder, \"district_routes_time_minutes.csv\")\n",
    "dist_dist_matrix = os.path.join(ma_folder, \"district_routes_distance_km.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inAdmin = gpd.read_file(admin_bounds)\n",
    "\n",
    "if not os.path.exists(local_population):\n",
    "    globalP = rasterio.open(global_population_file)\n",
    "    rMisc.clipRaster(globalP, inAdmin, local_population)\n",
    "\n",
    "if not os.path.exists(local_ghs_population):\n",
    "    globalP = rasterio.open(global_population_ghs_file)\n",
    "    rMisc.clipRaster(globalP, inAdmin, local_ghs_population)\n",
    "\n",
    "inP = rasterio.open(local_population)\n",
    "inP_ghs = rasterio.open(local_ghs_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run urbanization analysis\n",
    "1. Create urban extents  \n",
    "2. Calculate urban population in admin bounds  \n",
    "3. Summarize nighttime lights in extents and admin\n",
    "4. Summarize GHSL in extents and admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create urban extents for WorldPop\n",
    "if not os.path.exists(urban_extents_file):\n",
    "    urban_calculator = urban.urbanGriddedPop(inP)\n",
    "    urban_extents = urban_calculator.calculateUrban(\n",
    "        densVal=300,\n",
    "        totalPopThresh=5000,\n",
    "        smooth=False,\n",
    "        queen=False,\n",
    "        verbose=True,\n",
    "        raster=urban_extents_raster_file,\n",
    "    )\n",
    "    urban_extents_hd = urban_calculator.calculateUrban(\n",
    "        densVal=1500,\n",
    "        totalPopThresh=50000,\n",
    "        smooth=True,\n",
    "        queen=False,\n",
    "        verbose=True,\n",
    "        raster=urban_extents_hd_raster_file,\n",
    "    )\n",
    "    # Name urban extents\n",
    "    urban_extents = urban.geocode_cities(urban_extents)\n",
    "    urban_extents_hd = urban.geocode_cities(urban_extents_hd)\n",
    "    urban_extents.to_file(urban_extents_file, driver=\"GeoJSON\")\n",
    "    urban_extents_hd.to_file(urban_extents_hd_file, driver=\"GeoJSON\")\n",
    "else:\n",
    "    urban_extents = gpd.read_file(urban_extents_file)\n",
    "    urban_extents_hd = gpd.read_file(urban_extents_hd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b. Create urban extents for GHS_Pop\n",
    "if not os.path.exists(urban_extents_file):\n",
    "    urban_calculator = urban.urbanGriddedPop(inP_ghs)\n",
    "    urban_extents = urban_calculator.calculateUrban(\n",
    "        densVal=300,\n",
    "        totalPopThresh=5000,\n",
    "        smooth=False,\n",
    "        queen=False,\n",
    "        verbose=True,\n",
    "        raster=urban_extents_raster_file,\n",
    "    )\n",
    "    if urban_extents.crs.to_epsg() != 4326:\n",
    "        urban_extents = urban_extents.to_crs(4326)\n",
    "    urban_extents = urban.geocode_cities(urban_extents)\n",
    "    urban_extents.to_file(urban_extents_file, driver=\"GeoJSON\")\n",
    "if not os.path.exists(urban_extents_hd_file):\n",
    "    urban_extents_hd = urban_calculator.calculateUrban(\n",
    "        densVal=1500,\n",
    "        totalPopThresh=50000,\n",
    "        smooth=True,\n",
    "        queen=False,\n",
    "        verbose=True,\n",
    "        raster=urban_extents_hd_raster_file,\n",
    "    )\n",
    "    if urban_extents_hd.crs.to_epsg() != 4326:\n",
    "        urban_extents_hd = urban_extents_hd.to_crs(4326)\n",
    "    # Name urban extents\n",
    "    urban_extents_hd = urban.geocode_cities(urban_extents_hd)\n",
    "    urban_extents_hd.to_file(urban_extents_hd_file, driver=\"GeoJSON\")\n",
    "else:\n",
    "    urban_extents = gpd.read_file(urban_extents_file)\n",
    "    urban_extents_hd = gpd.read_file(urban_extents_hd_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Calculate urban population in admin areas\n",
    "if not os.path.exists(urban_admin_summary):\n",
    "    pop_worker = clippy.summarize_population(\n",
    "        local_ghs_population,\n",
    "        inAdmin,\n",
    "        urban_extents_raster_file,\n",
    "        urban_extents_hd_raster_file,\n",
    "    )\n",
    "    summarized_urban = pop_worker.calculate_zonal()\n",
    "    urban_res = summarized_urban.loc[\n",
    "        :, [x for x in summarized_urban.columns if \"SUM\" in x]\n",
    "    ]\n",
    "    urban_res.columns = [\"TOTAL_POP\", \"URBAN_POP\", \"URBAN_HD_POP\"]\n",
    "    urban_res[\"district_c\"] = inAdmin[\"district_c\"]\n",
    "    urban_res[\"district\"] = inAdmin[\"district\"]\n",
    "    urban_res.to_csv(urban_admin_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. summarize nighttime lights\n",
    "ntl_files = ntl.aws_search_ntl()\n",
    "\n",
    "viirs_folder = os.path.join(output_data, \"NTL_ZONAL_RES\")\n",
    "if not os.path.exists(viirs_folder):\n",
    "    os.makedirs(viirs_folder)\n",
    "\n",
    "urbanD = gpd.read_file(urban_extents_file)\n",
    "urbanHD = gpd.read_file(urban_extents_hd_file)\n",
    "\n",
    "for ntl_file in ntl_files:\n",
    "    name = ntl_file.split(\"/\")[-1].split(\"_\")[2][:8]\n",
    "    inR = rasterio.open(ntl_file)\n",
    "    tPrint(\"Processing %s\" % name)\n",
    "    urban_res_file = os.path.join(viirs_folder, f\"URBAN_{name}.csv\")\n",
    "    urban_hd_res_file = os.path.join(viirs_folder, f\"HD_URBAN_{name}.csv\")\n",
    "    admin_res_file = os.path.join(viirs_folder, f\"ADMIN_{name}.csv\")\n",
    "\n",
    "    # Urban Summary\n",
    "    if not os.path.exists(urban_res_file):\n",
    "        urban_res = rMisc.zonalStats(urbanD, inR, minVal=0.1)\n",
    "        col_names = [f\"URBAN_{name}_{x}\" for x in [\"SUM\", \"MIN\", \"MAX\", \"MEAN\"]]\n",
    "        urban_df = pd.DataFrame(urban_res, columns=col_names)\n",
    "        urban_df.to_csv(urban_res_file)\n",
    "    # HD Urban Summary\n",
    "    if not os.path.exists(urban_hd_res_file):\n",
    "        hd_urban_res = rMisc.zonalStats(urbanHD, inR, minVal=0.1)\n",
    "        col_names = [f\"HD_URBAN_{name}_{x}\" for x in [\"SUM\", \"MIN\", \"MAX\", \"MEAN\"]]\n",
    "        hd_urban_df = pd.DataFrame(hd_urban_res, columns=col_names)\n",
    "        hd_urban_df.to_csv(urban_hd_res_file)\n",
    "    # admin Summary\n",
    "    if not os.path.exists(admin_res_file):\n",
    "        admin_res = rMisc.zonalStats(inAdmin, inR, minVal=0.1)\n",
    "        col_names = [f\"ADM_URBAN_{name}_{x}\" for x in [\"SUM\", \"MIN\", \"MAX\", \"MEAN\"]]\n",
    "        admin_df = pd.DataFrame(admin_res, columns=col_names)\n",
    "        admin_df.to_csv(admin_res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile VIIRS results\n",
    "urb_files = [x for x in os.listdir(viirs_folder) if x.startswith(\"URBAN\")]\n",
    "for x in urb_files:\n",
    "    tempD = pd.read_csv(os.path.join(viirs_folder, x), index_col=0)\n",
    "    urbanD[x[:-4]] = tempD.iloc[:, 0]\n",
    "\n",
    "hd_urb_files = [x for x in os.listdir(viirs_folder) if x.startswith(\"HD_URBAN\")]\n",
    "for x in hd_urb_files:\n",
    "    tempD = pd.read_csv(os.path.join(viirs_folder, x), index_col=0)\n",
    "    urbanHD[x[:-4]] = tempD.iloc[:, 0]\n",
    "\n",
    "admin_urb_files = [x for x in os.listdir(viirs_folder) if x.startswith(\"ADMIN\")]\n",
    "for x in admin_urb_files:\n",
    "    tempD = pd.read_csv(os.path.join(viirs_folder, x), index_col=0)\n",
    "    inAdmin[x[:-4]] = tempD.iloc[:, 0]\n",
    "\n",
    "urbanD.drop([\"geometry\"], axis=1).to_csv(urban_viirs_summary)\n",
    "urbanHD.drop([\"geometry\"], axis=1).to_csv(urban_hd_viirs_summary)\n",
    "inAdmin.drop([\"geometry\"], axis=1).to_csv(admin_viirs_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize GHSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in ghsl folder\n",
    "ghsl_files = [\n",
    "    os.path.join(global_ghsl_folder, x)\n",
    "    for x in os.listdir(global_ghsl_folder)\n",
    "    if x.endswith(\".tif\")\n",
    "]\n",
    "\n",
    "for file_def in [\n",
    "    # [admin_bounds, admin_ghsl_summary],\n",
    "    [urban_extents_file, urban_ghsl_summary],\n",
    "    [urban_extents_file, urban_ghsl_summary],\n",
    "]:\n",
    "    resG = gpd.read_file(file_def[0])\n",
    "\n",
    "    for ghsl_file in ghsl_files:\n",
    "        date = os.path.basename(ghsl_file).split(\"_\")[3]\n",
    "        inR = rasterio.open(ghsl_file)\n",
    "        if resG.crs != inR.crs:\n",
    "            resG = resG.to_crs(inR.crs)\n",
    "        local_file = os.path.join(ghsl_folder, os.path.basename(ghsl_file))\n",
    "        if not os.path.exists(local_file):\n",
    "            rMisc.clipRaster(inR, resG, local_file)\n",
    "        res = rMisc.zonalStats(resG, inR, minVal=0)\n",
    "        res = pd.DataFrame(res, columns=[\"SUM\", \"MIN\", \"MAX\", \"MEAN\"])\n",
    "        resG[f\"ghsl_{date}\"] = res[\"SUM\"]\n",
    "        print(date)\n",
    "    pd.DataFrame(resG.drop([\"geometry\"], axis=1)).to_csv(file_def[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join urban clusters and districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inUrban = gpd.read_file(urban_extents_file)\n",
    "inHD = gpd.read_file(urban_extents_hd_file)\n",
    "inAdmin = gpd.read_file(admin_bounds)\n",
    "inAdmin = inAdmin.to_crs(inUrban.crs)\n",
    "\n",
    "ntl_files = ntl.aws_search_ntl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority_polygon(\n",
    "    shp, shp2, pop_layer, area_name=\"zz_area\", zonal_sum_name=\"zz_sum\"\n",
    "):\n",
    "    \"\"\"Intersect shp(single polygon) with shp2(GeoDataFrame) to determine which row in shp2 has\n",
    "        the highest zonal sum (ie - population)\n",
    "\n",
    "    Args:\n",
    "        shp: shapely polygon\n",
    "        shp2: GeoDataFrame\n",
    "        pop_layer: rasterio reader\n",
    "    returns\n",
    "        shp2 GeoDataFrame with two additional columns: area and zonal_sum\n",
    "    \"\"\"\n",
    "    temp_shp = shp2.copy()\n",
    "    for idx, row in temp_shp.iterrows():\n",
    "        # Convert geometry in shp2 to the intersection with shp1\n",
    "        xx = row[\"geometry\"].intersection(shp.buffer(0)).buffer(0)\n",
    "        temp_shp.loc[[idx], \"geometry\"] = gpd.GeoDataFrame(\n",
    "            geometry=[xx]\n",
    "        ).geometry.values\n",
    "\n",
    "    # Run zonal analysis on pop_layer\n",
    "    res = rMisc.zonalStats(temp_shp, pop_layer, reProj=True)\n",
    "    res = pd.DataFrame(res, columns=[\"SUM\", \"MIN\", \"MAX\", \"MEAN\"])\n",
    "\n",
    "    shp2[zonal_sum_name] = res[\"SUM\"].values\n",
    "    shp2[area_name] = temp_shp[\"geometry\"].apply(lambda x: x.area)\n",
    "\n",
    "    return shp2.sort_values(zonal_sum_name, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inUrban[\"HD_ID\"] = \"\"\n",
    "inUrban[\"Admin1_ID\"] = \"\"\n",
    "inUrban[\"Admin1_Pop\"] = 0\n",
    "inUrban[\"Admin2_ID\"] = \"\"\n",
    "inUrban[\"Admin2_Pop\"] = 0\n",
    "\n",
    "\n",
    "for idx, row in inUrban.iterrows():\n",
    "    tPrint(idx)\n",
    "    # Identify intersecting HD urban areas\n",
    "    selHD = inHD.loc[inHD.intersects(row[\"geometry\"])]\n",
    "    if selHD.shape[0] == 1:\n",
    "        inUrban.loc[idx, \"HD_ID\"] = selHD[\"ID\"].iloc[0]\n",
    "    elif selHD.shape[0] > 1:\n",
    "        selHD = get_majority_polygon(row[\"geometry\"], selHD, inP_ghs)\n",
    "        inUrban.loc[idx, \"HD_ID\"] = selHD[\"ID\"].iloc[0]\n",
    "\n",
    "    # Identify intersecting admin areas\n",
    "    selAdmin = inAdmin.loc[inAdmin.intersects(row[\"geometry\"])]\n",
    "    if selAdmin.shape[0] == 1:\n",
    "        inUrban.loc[idx, \"Admin1_ID\"] = selAdmin[\"district_c\"].iloc[0]\n",
    "    elif selAdmin.shape[0] > 1:\n",
    "        selAdmin = get_majority_polygon(row[\"geometry\"], selAdmin, inP_ghs)\n",
    "        inUrban.loc[idx, \"Admin1_ID\"] = selAdmin[\"district_c\"].iloc[0]\n",
    "        inUrban.loc[idx, \"Admin1_Pop\"] = selAdmin[\"zz_sum\"].iloc[0]\n",
    "        inUrban.loc[idx, \"Admin2_ID\"] = selAdmin[\"district_c\"].iloc[1]\n",
    "        inUrban.loc[idx, \"Admin2_Pop\"] = selAdmin[\"zz_sum\"].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inUrban.to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(inUrban.drop([\"geometry\"], axis=1)).to_csv(\n",
    "    urban_extents_file.replace(\".geojson\", \"_named.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate admin centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inAdmin_centroids = inAdmin.copy()\n",
    "for idx, row in inAdmin.iterrows():\n",
    "    # create numpy array of intersecting population layer\n",
    "    curP, transform = mask(inR, [row.geometry], crop=True)\n",
    "    curP[curP < 0] = 0\n",
    "    centroid_coords = ndimage.center_of_mass(curP)\n",
    "\n",
    "    b = row.geometry.bounds\n",
    "    x_range = b[2] - b[0]\n",
    "    y_range = b[3] - b[1]\n",
    "\n",
    "    x_coord = b[0] + x_range * (centroid_coords[1] / curP.shape[1])\n",
    "    y_coord = b[1] + y_range * (centroid_coords[2] / curP.shape[2])\n",
    "    final_geom = Point(x_coord, y_coord)\n",
    "    inAdmin_centroids.loc[idx, \"geometry\"] = final_geom\n",
    "inAdmin_centroids.to_file(admin_final.replace(\".shp\", \"_centroids.shp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(local_friction):\n",
    "    globalP = rasterio.open(global_friction_surface)\n",
    "    rMisc.clipRaster(globalP, inAdmin, local_friction)\n",
    "\n",
    "dests = gpd.read_file(urban_extents_file)\n",
    "dests[\"geometry\"] = dests[\"geometry\"].apply(lambda x: x.centroid)\n",
    "inR = rasterio.open(local_friction)\n",
    "frictionD = inR.read()[0, :, :]\n",
    "frictionD = frictionD * 1000\n",
    "mcp = graph.MCP_Geometric(frictionD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ma)\n",
    "# Calculate travel time between all urban areas\n",
    "all_rts = ma.get_linear_routes(inR, frictionD, dests, dests, \"ID\", \"ID\", verbose=True)\n",
    "all_rts = all_rts.to_crs(3857)\n",
    "all_rts[\"length_km\"] = all_rts[\"geometry\"].apply(lambda x: x.length / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_rts.to_file(all_routes_file)\n",
    "pd.pivot_table(all_rts, \"cost\", \"origin\", \"destination\").to_csv(time_matrix)\n",
    "pd.pivot_table(all_rts, \"length_km\", \"origin\", \"destination\").to_csv(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inAdmin_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ma)\n",
    "# Calculate travel time between all urban areas and district centroids\n",
    "inAdmin_centroids = gpd.read_file(admin_final.replace(\".shp\", \"_centroids.shp\"))\n",
    "dist_all_rts = ma.get_linear_routes_mp(\n",
    "    inR, frictionD, dests, inAdmin_centroids, \"ID\", \"district_c\", verbose=True\n",
    ")\n",
    "dist_all_rts = dist_all_rts.to_crs(3857)\n",
    "dist_all_rts[\"length_km\"] = dist_all_rts[\"geometry\"].apply(lambda x: x.length / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_all_rts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_all_rts.to_file(dist_all_routes_file)\n",
    "pd.pivot_table(dist_all_rts, \"cost\", \"origin\", \"destination\").to_csv(dist_time_matrix)\n",
    "pd.pivot_table(dist_all_rts, \"length_km\", \"origin\", \"destination\").to_csv(\n",
    "    dist_dist_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each urban area, generate a travel time to the centroid, and then sample for all the other areas\n",
    "urban_output_matrix = np.zeros([dests.shape[0], dests.shape[0]])\n",
    "for idx, row in dests.iterrows():\n",
    "    costs, trace = ma.calculate_travel_time(inR, mcp, row.to_frame().transpose())\n",
    "    cur_res = dests[\"geometry\"].apply(lambda x: costs[inR.index(x.x, x.y)])\n",
    "    output_matrix[idx,] = cur_res\n",
    "    tPrint(f\"{idx} of {dests.shape[0]} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_res = pd.DataFrame(output_matrix, columns=[f\"urb_{x}\" for x in dests[\"ID\"]])\n",
    "tt_res.index = dests[\"ID\"]\n",
    "tt_res.to_csv(urban_tt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attach IDs from ECA database to KAZ database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanD = gpd.read_file(urban_extents_file)\n",
    "eca_kaz = gpd.read_file(\n",
    "    \"/home/wb411133/projects/KAZ_SCADR_Urbanization/DATA/ECA_Extents/KAZ/KAZ_urban_extents.geojson\"\n",
    ")\n",
    "urbanD[\"eca_id\"] = 0\n",
    "for idx, row in urbanD.iterrows():\n",
    "    sel_eca = eca_kaz.loc[eca_kaz.intersects(row[\"geometry\"].centroid)]\n",
    "    if sel_eca.shape[0] > 0:\n",
    "        urbanD.loc[idx, \"eca_id\"] = sel_eca[\"ID\"].iloc[0]\n",
    "urbanD.to_file(urban_extents_file, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanHD = gpd.read_file(urban_extents_hd_file)\n",
    "eca_kaz = gpd.read_file(\n",
    "    \"/home/wb411133/projects/KAZ_SCADR_Urbanization/DATA/ECA_Extents/KAZ/KAZ_urban_extents_hd.geojson\"\n",
    ")\n",
    "urbanHD[\"eca_id\"] = 0\n",
    "for idx, row in urbanHD.iterrows():\n",
    "    sel_eca = eca_kaz.loc[eca_kaz.intersects(row[\"geometry\"].centroid)]\n",
    "    if sel_eca.shape[0] > 0:\n",
    "        urbanHD.loc[idx, \"eca_id\"] = sel_eca[\"ID\"].iloc[0]\n",
    "urbanHD.to_file(urban_extents_hd_file, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_filepath = gpd.datasets.get_path(\"naturalearth_lowres\")\n",
    "world = gpd.read_file(world_filepath)\n",
    "sel_country = world.loc[world[\"name\"] == \"Kenya\"]\n",
    "\n",
    "local_friction = \"/home/wb411133/temp/KEN_friction.tif\"\n",
    "if not os.path.exists(local_friction):\n",
    "    globalP = rasterio.open(global_friction_surface)\n",
    "    rMisc.clipRaster(globalP, sel_country, local_friction)\n",
    "\n",
    "inAdmin = gpd.read_file(\n",
    "    \"/home/public/Data/COUNTRY/KEN/ADMIN/geoBoundaries-KEN-ADM1.geojson\"\n",
    ")\n",
    "inAdmin_centroids = inAdmin.copy()\n",
    "inAdmin_centroids[\"geometry\"] = inAdmin_centroids[\"geometry\"].apply(\n",
    "    lambda x: x.centroid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inR = rasterio.open(local_friction)\n",
    "frictionD = inR.read()[0, :, :]\n",
    "frictionD = frictionD * 1000\n",
    "mcp = graph.MCP_Geometric(frictionD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ma)\n",
    "\n",
    "all_rts = ma.get_linear_routes(\n",
    "    inR,\n",
    "    frictionD,\n",
    "    inAdmin_centroids,\n",
    "    inAdmin_centroids,\n",
    "    \"shapeName\",\n",
    "    \"shapeName\",\n",
    "    verbose=True,\n",
    ")\n",
    "all_rts = all_rts.to_crs(3857)\n",
    "all_rts[\"length_km\"] = all_rts[\"geometry\"].apply(lambda x: x.length / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map resulting route\n",
    "centre = sel_country.unary_union.centroid\n",
    "m = folium.Map(location=[centre.y, centre.x], zoom_start=4)\n",
    "orig_map = inAdmin_centroids.iloc[0]\n",
    "rts = folium.GeoJson(\n",
    "    mapping(all_rts.unary_union),\n",
    "    style_function=lambda feature: {\"color\": \"red\", \"weight\": 1},\n",
    ")\n",
    "\n",
    "folium.CircleMarker(\n",
    "    location=[orig_map.geometry.y, orig_map.geometry.x],\n",
    "    radius=2,\n",
    "    weight=4,\n",
    "    color=\"blue\",\n",
    ").add_to(m)\n",
    "\n",
    "rts.add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot(all_rts, \"origin\", \"destination\", \"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot(all_rts, \"origin\", \"destination\", \"length_km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate h3 grid around dedicated city\n",
    "m = folium.Map(location=[row.geometry.y, row.geometry.x], zoom_start=4)\n",
    "\n",
    "folium.CircleMarker(\n",
    "    location=[y_range[0], x_range[0]], radius=2, weight=4, color=\"red\"\n",
    ").add_to(m)\n",
    "\n",
    "folium.CircleMarker(\n",
    "    location=[y_range[0], x_range[-1]], radius=2, weight=4, color=\"blue\"\n",
    ").add_to(m)\n",
    "\n",
    "folium.CircleMarker(\n",
    "    location=[y_range[-1], x_range[-1]], radius=2, weight=4, color=\"orange\"\n",
    ").add_to(m)\n",
    "\n",
    "folium.CircleMarker(\n",
    "    location=[y_range[-1], x_range[0]], radius=2, weight=4, color=\"green\"\n",
    ").add_to(m)\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
