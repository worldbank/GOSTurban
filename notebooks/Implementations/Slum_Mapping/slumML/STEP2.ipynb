{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng-6APvaCTjz"
   },
   "source": [
    "# **STEP-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lY0lVSWE86u"
   },
   "source": [
    "**OPTIONAL:** Install required modules (geopandas, pyproj, h2o) if not yet installed.\n",
    "Create 'modules' directory under 'Colab\\ Notebooks' (or specify a directory you like).\n",
    "\n",
    "https://ggcs.io/2020/06/22/google-colab-pip-install/ (Japanese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --target /content/drive/MyDrive/Colab\\ Notebooks/modules geopandas pyproj h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPbQEJvZIXas"
   },
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import pyproj\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from h2o.frame import H2OFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpsPI01sCAU_"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Initial file setting\n",
    "Do not forget to mount your GoogleDrive! All necessary files need to be uploaded in directories on your GoogleDrive. You can easily mount your GoogleDrive from the left pane, click the folder icon and select mount drive. Also see: https://colab.research.google.com/notebooks/io.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"/content/drive/MyDrive/Colab Notebooks/slumML/data/Yaounde/\"  # Directory to save model, outputs\n",
    "building_file = \"/content/drive/MyDrive/Colab Notebooks/slumML/data/Yaounde/Yaounde_DA_morphology.shp\"  # Specify the processed building footprint data\n",
    "sample_file = \"/content/drive/MyDrive/Colab Notebooks/slumML/data/Yaounde/Yaounde_sample_data.shp\"  # Specify the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a Building Footprint layer processed at STEP-1\n",
    "proj_4326 = pyproj.Proj(4326)\n",
    "\n",
    "building_df = gpd.read_file(building_file)\n",
    "building_df = building_df.to_crs(4326)\n",
    "\n",
    "# Read a Sample Area layer\n",
    "sample_area = gpd.read_file(sample_file)\n",
    "sample_area = sample_area.to_crs(4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS0NcJ-VSCbj"
   },
   "source": [
    "Here, adjust your prediction and response variables.\n",
    "Modify the code below to satisfy your needs.Current setting is very basic: Apply all variables in the building_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urban classes to be used in the sample layer and for classification\n",
    "# Assign unique integer for each class by yourself here.\n",
    "# class_map = {'High Income':1,'Middle income':2,'Industrial':5, 'Informal':3, 'Commercial':4}\n",
    "# class_map = {'High income':1,'Middle income':2,'Informal':3, 'Commercial':4}\n",
    "# class_map = {'commercial':1,'industrial':2,'ins_admin':3, 'res':4, 'res_detached':5, 'slum':6}#Nairobi\n",
    "# class_map = {'Commercial zone':1,'Formal':2,'Informal':3, 'middle income':4}#Bangui\n",
    "# class_map = {'High Income':1,'Middle Income':2,'Low Income':3, 'Slum':4, 'Com Admin':5, 'Industrial':6}#Libreville, Brazzaville, & PointeNoire\n",
    "# class_map = {'commercial':1,'informal':2,'mid/low income':3}#Bambari\n",
    "class_map = {\n",
    "    \"Administrative\": 0,\n",
    "    \"Bidonville\": 1,\n",
    "    \"Commercial\": 2,\n",
    "    \"High income\": 3,\n",
    "    \"Industrial\": 4,\n",
    "    \"Middle/Low income\": 5,\n",
    "}  # Douala, Yaounde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, adjust your prediction and response variables. Modify the code below to satisfy your needs.\n",
    "# Current setting is very basic: Apply all variables in the building_df.\n",
    "col = building_df.columns\n",
    "predictors = list(col[1:22])\n",
    "response = \"type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL ###\n",
    "# The col defining the urban space type should have 'type' name. So, if not, rename it here.\n",
    "sample_area = sample_area.rename(columns={\"class\": \"type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a training data by intersecting 'building_df' and 'sample_area'\n",
    "# Set urban class default as 'unknown'\n",
    "\n",
    "source_df = building_df.copy()\n",
    "\n",
    "source_df[\"type\"] = \"unknown\"\n",
    "\n",
    "# Create an empty DF for append\n",
    "training_data = pd.DataFrame()\n",
    "\n",
    "# 'training_data' is now our official 'training data' for the ML model.\n",
    "for index, row in sample_area.iterrows():\n",
    "    x = row.geometry\n",
    "    y = row.type\n",
    "\n",
    "    df_temp = source_df[source_df.intersects(x)].copy()\n",
    "    df_temp[\"type\"] = y\n",
    "\n",
    "    training_data = training_data.append(df_temp)\n",
    "\n",
    "training_data[\"type\"] = training_data[\"type\"].map(class_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82ku5ND2SOWr"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# **h2o Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvN08ZHeTSmd"
   },
   "source": [
    "Initialize h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICa4GXDWTY1o"
   },
   "source": [
    "Data prep for the h2o pipeline, autoML model selection.\n",
    "The best-performed model will be saved in the directory specified by 'pth.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the training data to an h2o frame.\n",
    "# NOTE that this process will be inefficien if the original data has many NaNs.\n",
    "hf = H2OFrame(training_data)\n",
    "\n",
    "\n",
    "# This block of code is fairly h2o standard. It trains 20 models on this data,\n",
    "# limiting the runtime to 1 hour. At the end of an hour or training 20 models,\n",
    "# whichever is first, it returns a DataFrame of predictions as preds, ordered by the quality of their predictions.\n",
    "\n",
    "# Split 'hf' into a taraining frame and validation frame.\n",
    "train, valid = hf.split_frame(ratios=[0.8], seed=10)\n",
    "\n",
    "# Identify predictors and response\n",
    "x = predictors\n",
    "y = response\n",
    "\n",
    "## For binary classification, response should be a factor\n",
    "train[y] = train[y].asfactor()\n",
    "valid[y] = valid[y].asfactor()\n",
    "\n",
    "# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\n",
    "aml = H2OAutoML(max_models=20, seed=1)\n",
    "aml.train(x=x, y=y, training_frame=train)\n",
    "\n",
    "# View the AutoML Leaderboard\n",
    "lb = aml.leaderboard\n",
    "\n",
    "\n",
    "# Print all rows instead of default (10 rows)\n",
    "lb.head(rows=lb.nrows)\n",
    "\n",
    "print(\"** Model validation with 'valid' hf **\")\n",
    "preds = aml.leader.predict(valid)\n",
    "\n",
    "# Here, we print out the performance of our top performing model.\n",
    "res = aml.leader.model_performance(valid)\n",
    "\n",
    "print(res)\n",
    "\n",
    "# We save the model down to its own save location.\n",
    "model_path = h2o.save_model(model=aml.leader, path=pth, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnH6Xw8QT5ee"
   },
   "source": [
    "### **Supervised ML classification based on the selected model**\n",
    "h2o struggled to generate predictions for more than 100,000 rows at a time.Thus, we split the original DataFrame into 100,000 row chunks, run the predictions on the h2o version of the frame, then send these to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_row_size = 100000\n",
    "\n",
    "chunk_num = int(len(building_df) / max_row_size)\n",
    "chunk_mod = len(building_df) % max_row_size\n",
    "\n",
    "building_df[\"type\"] = \"unknown\"\n",
    "\n",
    "\n",
    "def MLpred(df):\n",
    "    df_input = df[predictors]\n",
    "    # Extract predictor cols only (specified by the 'predictors' LIST)\n",
    "    hf_temp = H2OFrame(df_input)\n",
    "\n",
    "    preds_temp = aml.leader.predict(hf_temp)\n",
    "    pred_df_temp = preds_temp.as_data_frame()\n",
    "\n",
    "    # add 'PID' to 'pred_df_temp' so that it will be merged to the original 'df.'\n",
    "    df.reset_index(inplace=True)\n",
    "    pred_df_temp[\"PID\"] = df.PID\n",
    "\n",
    "    ans = pd.merge(df, pred_df_temp, on=\"PID\")\n",
    "\n",
    "    return ans\n",
    "\n",
    "\n",
    "# Create an empty DF for append\n",
    "prediction_df = pd.DataFrame()\n",
    "\n",
    "# If the total number of building footprints is smaller than 100,000:\n",
    "if len(building_df) < 100000:\n",
    "    # Prediction process here\n",
    "    pred_x = MLpred(building_df)\n",
    "    prediction_df = prediction_df.append(pred_x)\n",
    "\n",
    "else:\n",
    "    for i in range(0, chunk_num):\n",
    "        if i == 0:\n",
    "            print(\"Processing Chunk No. {} ----> row 0–{}\".format(i + 1, max_row_size))\n",
    "            df_temp2 = building_df[0:max_row_size].copy()\n",
    "\n",
    "            # Prediction process here\n",
    "            pred_x = MLpred(df_temp2)\n",
    "\n",
    "            prediction_df = prediction_df.append(pred_x)\n",
    "\n",
    "        else:\n",
    "            start = i * max_row_size\n",
    "            stop = (i * max_row_size) + max_row_size\n",
    "            print(\"Processing Chunk No. {} ----> row {}–{}\".format(i + 1, start, stop))\n",
    "            df_temp2 = building_df[start:stop].copy()\n",
    "\n",
    "            # Prediction process here\n",
    "            pred_x = MLpred(df_temp2)\n",
    "\n",
    "            prediction_df = prediction_df.append(pred_x)\n",
    "\n",
    "    if chunk_mod > 0:\n",
    "        start = chunk_num * max_row_size\n",
    "        print(\"Processing Chunk No. {} ----> row {} till the end\".format(i + 1, start))\n",
    "        df_temp2 = building_df[start:].copy()\n",
    "\n",
    "        # Prediction process here\n",
    "        pred_x = MLpred(df_temp2)\n",
    "\n",
    "        prediction_df = prediction_df.append(pred_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqwpmSsrUIY_"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Post-analytical process**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exporting\n",
    "print(\"Exporting reulst to shapefile...\")\n",
    "output_path = pth + \"\\prediction_result.shp\"\n",
    "prediction_df.to_file(output_path, driver=\"ESRI Shapefile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refreshing H2O cluster (if necessary)\n",
    "h2o.shutdown(prompt=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
